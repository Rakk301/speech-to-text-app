---
alwaysApply: true
---

# Speech-to-Text Application Overview

This is a macOS-native speech-to-text utility app that runs offline, triggered by a global hotkey (âŒ˜â‡§S), transcribes real-time microphone input, applies LLM-based postprocessing, then pastes the final output at the cursor.

## ğŸ—ï¸ System Architecture

### Core Components
- **Swift**: System integration (hotkeys, audio capture, paste operations)
- **Python**: ML inference (Whisper STT, LLM postprocessing)
- **Bridge**: Subprocess communication between Swift and Python

### Data Flow
1. Global hotkey pressed â†’ Swift records audio
2. Swift saves audio to file â†’ calls Python script
3. Python runs Whisper STT â†’ LLM postprocessing â†’ stdout
4. Swift receives text â†’ pastes at cursor â†’ logs with timestamp

## ğŸ“ Expected Project Structure
```
Speech To Text App/
â”œâ”€â”€ Swift/
â”‚   â”œâ”€â”€ SpeechToTextApp.swift      # Main Swift app
â”‚   â”œâ”€â”€ AudioRecorder.swift        # AVAudioEngine wrapper
â”‚   â”œâ”€â”€ HotkeyManager.swift        # Global hotkey handling
â”‚   â””â”€â”€ PasteManager.swift         # Clipboard and paste operations
â”œâ”€â”€ Python/
â”‚   â”œâ”€â”€ transcribe.py              # Main STT script
â”‚   â”œâ”€â”€ whisper_wrapper.py         # Whisper integration
â”‚   â”œâ”€â”€ llm_processor.py           # LLM postprocessing
â”‚   â””â”€â”€ config.py                  # Configuration management
â”œâ”€â”€ Config/
â”‚   â””â”€â”€ settings.yaml              # User-configurable settings
â””â”€â”€ Logs/
    â””â”€â”€ transcriptions.log         # Timestamped transcription logs
```

## ğŸ¯ Development Principles
- Keep Swift side lean and focused on system integration
- Use Python for all ML/LLM operations
- Maintain loose coupling via subprocess communication
- CLI-style Python scripts for easy testing and iteration
---
# Speech-to-Text Application Overview

This is a macOS-native speech-to-text utility app that runs offline, triggered by a global hotkey (âŒ˜â‡§S), transcribes real-time microphone input, applies LLM-based postprocessing, then pastes the final output at the cursor.

## ğŸ—ï¸ System Architecture

### Core Components
- **Swift**: System integration (hotkeys, audio capture, paste operations)
- **Python**: ML inference (Whisper STT, LLM postprocessing)
- **Bridge**: Subprocess communication between Swift and Python

### Data Flow
1. Global hotkey pressed â†’ Swift records audio
2. Swift saves audio to file â†’ calls Python script
3. Python runs Whisper STT â†’ LLM postprocessing â†’ stdout
4. Swift receives text â†’ pastes at cursor â†’ logs with timestamp

## ğŸ“ Expected Project Structure
```
Speech To Text App/
â”œâ”€â”€ Swift/
â”‚   â”œâ”€â”€ SpeechToTextApp.swift      # Main Swift app
â”‚   â”œâ”€â”€ AudioRecorder.swift        # AVAudioEngine wrapper
â”‚   â”œâ”€â”€ HotkeyManager.swift        # Global hotkey handling
â”‚   â””â”€â”€ PasteManager.swift         # Clipboard and paste operations
â”œâ”€â”€ Python/
â”‚   â”œâ”€â”€ transcribe.py              # Main STT script
â”‚   â”œâ”€â”€ whisper_wrapper.py         # Whisper integration
â”‚   â”œâ”€â”€ llm_processor.py           # LLM postprocessing
â”‚   â””â”€â”€ config.py                  # Configuration management
â”œâ”€â”€ Config/
â”‚   â””â”€â”€ settings.yaml              # User-configurable settings
â””â”€â”€ Logs/
    â””â”€â”€ transcriptions.log         # Timestamped transcription logs
```

## ğŸ¯ Development Principles
- Keep Swift side lean and focused on system integration
- Use Python for all ML/LLM operations
- Maintain loose coupling via subprocess communication
- CLI-style Python scripts for easy testing and iteration
---
